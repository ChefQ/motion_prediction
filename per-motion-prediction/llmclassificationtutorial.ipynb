{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset , load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding , AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification , TrainingArguments , AutoConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "import random \n",
    "from transformers import TrainingArguments, Trainer , AutoModelForSequenceClassification\n",
    "\n",
    "roberta_checkpoint = \"roberta-large\"\n",
    "mistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"\n",
    "llama_checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
    "MAX_LEN = 512 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = \"mistralai/Mistral-7B-v0.1\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "max_input_size =  config.max_position_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"mehdiiraqui/twitter_disaster\")\n",
    "\n",
    "# Split the dataset into training and validation datasets\n",
    "data = dataset['train'].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "data['val'] = data.pop(\"test\")\n",
    "# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\n",
    "data['test'] = dataset['test']\n",
    "\n",
    "col_to_delete = ['id', 'keyword','location', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6090 entries, 0 to 6089\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        6090 non-null   int64 \n",
      " 1   keyword   6037 non-null   object\n",
      " 2   location  4064 non-null   object\n",
      " 3   text      6090 non-null   object\n",
      " 4   target    6090 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 238.0+ KB\n",
      "test\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      " 4   target    3263 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 127.6+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"train\")\n",
    "\n",
    "data['train'].to_pandas().info()\n",
    "\n",
    "print(\"test\")\n",
    "\n",
    "data['test'].to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[1])\n",
    "\n",
    "neg_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1622137404580153, 0.877521613832853)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weights, neg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char = data['train'].to_pandas()['text'].str.len().max()\n",
    "max_words = data['train'].to_pandas()['text'].str.split().str.len().max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "each model has their specific tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5285,\n",
       " 'keyword': 'fear',\n",
       " 'location': 'Thibodaux, LA',\n",
       " 'text': 'my worst fear. https://t.co/iH8UDz8mq3',\n",
       " 'target': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_checkpoint, add_prefix_space=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_preprocessing_function(examples):\n",
    "    return roberta_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_preprocessing_function(data['train'][0])\n",
    "\n",
    "\n",
    "# Apply the preprocessing function and remove the undesired columns\n",
    "roberta_tokenized_datasets = data.map(roberta_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# Rename the target to label as for HugginFace standards\n",
    "roberta_tokenized_datasets = roberta_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "# Set to torch format\n",
    "roberta_tokenized_datasets.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(0),\n",
       " 'input_ids': tensor([    0,   993,  5373, 14489,   751,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenized_datasets['train'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "roberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmistral_preprocessing_function\u001b[39m(examples):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mistral_tokenizer(examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39mMAX_LEN)\n\u001b[0;32m----> 9\u001b[0m mistral_tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mmap(mistral_preprocessing_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39mcol_to_delete)\n\u001b[1;32m     10\u001b[0m mistral_tokenized_datasets \u001b[38;5;241m=\u001b[39m mistral_tokenized_datasets\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m mistral_tokenized_datasets\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint, add_prefix_space=True)\n",
    "mistral_tokenizer.pad_token_id = mistral_tokenizer.eos_token_id\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "\n",
    "\n",
    "def mistral_preprocessing_function(examples):\n",
    "    return mistral_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "mistral_tokenized_datasets = data.map(mistral_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "mistral_tokenized_datasets = mistral_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "mistral_tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "mistral_data_collator = DataCollatorWithPadding(tokenizer=mistral_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama 2\n",
    "\n",
    "couldn't use LLama 2 because i don't have a license yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRA setup for RoBERTa classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,248,258 || all params: 356,610,052 || trainable%: 0.35003444042009224\n"
     ]
    }
   ],
   "source": [
    "roberta_peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n",
    ")\n",
    "roberta_model = get_peft_model(roberta_model, roberta_peft_config)\n",
    "roberta_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mistral classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fd57034d22468c8d0654e658bfb449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mistral_model =  AutoModelForSequenceClassification.from_pretrained(\n",
    "  pretrained_model_name_or_path=mistral_checkpoint,\n",
    "  num_labels=2,\n",
    "  device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 860,160 || all params: 7,111,528,448 || trainable%: 0.012095290151611583\n"
     ]
    }
   ],
   "source": [
    "#For Mistral 7B, we have to add the padding token id as it is not defined by default.\n",
    "mistral_model.config.pad_token_id = mistral_model.config.eos_token_id\n",
    "\n",
    "mistral_peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\", \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# For Mistral 7B model, we need to specify the target_modules (the query and value vectors from the attention modules)\n",
    "mistral_model = get_peft_model(mistral_model, mistral_peft_config)\n",
    "mistral_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # All metrics are already predefined in the HF `evaluate` package\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric= evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class WeightedCELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Get model's predictions\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Compute custom loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roberta_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m roberta_model \u001b[38;5;241m=\u001b[39m \u001b[43mroberta_model\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      2\u001b[0m roberta_model\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mNameError\u001b[0m: name 'roberta_model' is not defined"
     ]
    }
   ],
   "source": [
    "roberta_model = roberta_model.cuda()\n",
    "roberta_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"roberta-large-lora-token-classification\",\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type= \"constant\",\n",
    "    warmup_ratio= 0.1,\n",
    "    max_grad_norm= 0.3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.001,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roberta_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m roberta_trainer \u001b[38;5;241m=\u001b[39m WeightedCELossTrainer(\n\u001b[0;32m----> 2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mroberta_model\u001b[49m,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mroberta_tokenized_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mroberta_tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mroberta_data_collator,\n\u001b[1;32m      7\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'roberta_model' is not defined"
     ]
    }
   ],
   "source": [
    "roberta_trainer = WeightedCELossTrainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=roberta_tokenized_datasets['train'],\n",
    "    eval_dataset=roberta_tokenized_datasets[\"val\"],\n",
    "    data_collator=roberta_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weightedCELossTrainer for Mistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WeightedCELossTrainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m      9\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     11\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     12\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-lora-token-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     gradient_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m mistral_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mWeightedCELossTrainer\u001b[49m(\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m=\u001b[39mmistral_model,\n\u001b[1;32m     32\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     33\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mmistral_tokenized_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     34\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mmistral_tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     35\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mmistral_data_collator,\n\u001b[1;32m     36\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     40\u001b[0m mistral_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Try this tutotial:\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/docs/transformers/en/training\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# First try thier example, then try it with Lora Mistral\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WeightedCELossTrainer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#mistral_model = mistral_model.cuda()\n",
    "\n",
    "\n",
    "os .environ[\"WANDB_PROJECT\"] = \"LLM_TUTORIAL\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 11\n",
    "num_epochs = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mistral-lora-token-classification\",\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type= \"constant\",\n",
    "    warmup_ratio= 0.1,\n",
    "    max_grad_norm= 0.3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.001,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "   #fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "\n",
    "mistral_trainer = WeightedCELossTrainer(\n",
    "    model=mistral_model,\n",
    "    args=training_args,\n",
    "    train_dataset=mistral_tokenized_datasets['train'],\n",
    "    eval_dataset=mistral_tokenized_datasets[\"val\"],\n",
    "    data_collator=mistral_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "mistral_trainer.train()\n",
    "\n",
    "# Try this tutotial:\n",
    "# https://huggingface.co/docs/transformers/en/training\n",
    "# First try thier example, then try it with Lora Mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /u3/oqcardos/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moqcardoso\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/u3/oqcardos/motion_prediction/per-motion-prediction/wandb/run-20240209_230244-evcums1z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oqcardoso/first-tutorial-project/runs/evcums1z' target=\"_blank\">sleek-sun-2</a></strong> to <a href='https://wandb.ai/oqcardoso/first-tutorial-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oqcardoso/first-tutorial-project' target=\"_blank\">https://wandb.ai/oqcardoso/first-tutorial-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oqcardoso/first-tutorial-project/runs/evcums1z' target=\"_blank\">https://wandb.ai/oqcardoso/first-tutorial-project/runs/evcums1z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.01\n",
      "epoch=2, accuracy=0.5311958275063068, loss=0.5462571801698768\n",
      "epoch=3, accuracy=0.5770187346016662, loss=0.49816277707060924\n",
      "epoch=4, accuracy=0.7244618636398443, loss=0.32406454573591315\n",
      "epoch=5, accuracy=0.8136540458340896, loss=0.2785895298912673\n",
      "epoch=6, accuracy=0.7728565529432286, loss=0.12697897832344268\n",
      "epoch=7, accuracy=0.7716481220609013, loss=0.10790416359249613\n",
      "epoch=8, accuracy=0.8717955166000493, loss=0.11637631030992442\n",
      "epoch=9, accuracy=0.8818570223579603, loss=0.1250571506458179\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import wandb\n",
    "import random  # for demo script\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"first-tutorial-project\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "    },\n",
    ")\n",
    "\n",
    "offset = random.random() / 5\n",
    "print(f\"lr: {lr}\")\n",
    "\n",
    "# simulating a training run\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2**-epoch - random.random() / epoch - offset\n",
    "    loss = 2**-epoch + random.random() / epoch + offset\n",
    "    print(f\"epoch={epoch}, accuracy={acc}, loss={loss}\")\n",
    "    wandb.log({\"accuracy\": acc, \"loss\": loss})\n",
    "\n",
    "# run.log_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(output_dir=\"Mistral_test_trainer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19332f0514e140768653fd894bac8889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 872,448 || all params: 7,111,553,024 || trainable%: 0.012268037615070448\n"
     ]
    }
   ],
   "source": [
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint, add_prefix_space=True)\n",
    "mistral_tokenizer.pad_token_id = mistral_tokenizer.eos_token_id\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "\n",
    "def mistral_preprocessing_function(examples):\n",
    "    return mistral_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "mistral_tokenized_datasets = dataset.map(mistral_preprocessing_function, batched=True)\n",
    "mistral_tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "mistral_data_collator = DataCollatorWithPadding(tokenizer=mistral_tokenizer)\n",
    "\n",
    "small_train_dataset =   mistral_tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))   #bert_tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset =   mistral_tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "mistral_model =  AutoModelForSequenceClassification.from_pretrained(\n",
    "  pretrained_model_name_or_path=mistral_checkpoint,\n",
    "  num_labels=5,\n",
    "  device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "#For Mistral 7B, we have to add the padding token id as it is not defined by default.\n",
    "mistral_model.config.pad_token_id = mistral_model.config.eos_token_id\n",
    "\n",
    "\n",
    "mistral_peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\", \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# For Mistral 7B model, we need to specify the target_modules (the query and value vectors from the attention modules)\n",
    "mistral_model = get_peft_model(mistral_model, mistral_peft_config)\n",
    "mistral_model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moqcardoso\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/u3/oqcardos/motion_prediction/per-motion-prediction/wandb/run-20240210_023250-yqilkavy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL/runs/yqilkavy' target=\"_blank\">prosperous-rat-7</a></strong> to <a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL' target=\"_blank\">https://wandb.ai/oqcardoso/LLM_TUTORIAL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oqcardoso/LLM_TUTORIAL/runs/yqilkavy' target=\"_blank\">https://wandb.ai/oqcardoso/LLM_TUTORIAL/runs/yqilkavy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='206' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [206/375 19:30 < 16:09, 0.17 it/s, Epoch 1.64/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", report_to=\"wandb\")\n",
    "\n",
    "os .environ[\"WANDB_PROJECT\"] = \"LLM_TUTORIAL\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=mistral_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=mistral_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
