{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset , load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding , AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification , TrainingArguments , AutoConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "import random \n",
    "from transformers import TrainingArguments, Trainer , AutoModelForSequenceClassification\n",
    "\n",
    "roberta_checkpoint = \"roberta-large\"\n",
    "mistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"\n",
    "llama_checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
    "MAX_LEN = 512 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a228ba3760b942dca50a276bb41a1f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3e6534bc6a4094a2c43e83979a95ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/605 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion', 'brief_type', 'data_type', 'file_path', 'file_name', 'labels'],\n",
      "        num_rows: 610\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion', 'brief_type', 'data_type', 'file_path', 'file_name', 'labels'],\n",
      "        num_rows: 605\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>brief_type</th>\n",
       "      <th>data_type</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_name</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case 9:11-cv-80416-KLR Document 737 Entered on...</td>\n",
       "      <td>deny</td>\n",
       "      <td>support</td>\n",
       "      <td>train</td>\n",
       "      <td>1013</td>\n",
       "      <td>gov.uscourts.flsd.377721.737.0.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case 9:11-cv-80416-KLR Document 743 Entered on...</td>\n",
       "      <td>deny</td>\n",
       "      <td>opposition</td>\n",
       "      <td>train</td>\n",
       "      <td>1013</td>\n",
       "      <td>gov.uscourts.flsd.377721.743.0.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN THE UNITED STATES DISTRICT COURT\\nFOR THE S...</td>\n",
       "      <td>deny</td>\n",
       "      <td>support</td>\n",
       "      <td>train</td>\n",
       "      <td>909</td>\n",
       "      <td>gov.uscourts.mssd.95610.6.0.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I i .\\nt | | , : tl\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n...</td>\n",
       "      <td>deny</td>\n",
       "      <td>opposition</td>\n",
       "      <td>train</td>\n",
       "      <td>909</td>\n",
       "      <td>gov.uscourts.mssd.95610.7.0.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case 1:08-cv-11908-RWZ Document 27\\n\\nFiled 09...</td>\n",
       "      <td>grant</td>\n",
       "      <td>support</td>\n",
       "      <td>train</td>\n",
       "      <td>507</td>\n",
       "      <td>gov.uscourts.mad.118796.27.0.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>Case 6:10-cv-00111-LED Document 333\\n\\nFiled 0...</td>\n",
       "      <td>deny</td>\n",
       "      <td>opposition</td>\n",
       "      <td>test</td>\n",
       "      <td>621</td>\n",
       "      <td>gov.uscourts.txed.121829.333.0.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>Helio LLC v. Palm, Inc. Doc. 3\\n\\n \\n\\n \\n\\n \\...</td>\n",
       "      <td>deny</td>\n",
       "      <td>support</td>\n",
       "      <td>train</td>\n",
       "      <td>574</td>\n",
       "      <td>gov.uscourts.cand.187342.3.0.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>Helio LLC v. Palm, lnc.\\n\\n60373/2021440.1\\n\\n...</td>\n",
       "      <td>deny</td>\n",
       "      <td>opposition</td>\n",
       "      <td>train</td>\n",
       "      <td>574</td>\n",
       "      <td>gov.uscourts.cand.187342.9.0.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>Case 1:11-cv-00696-RLW Document 4 Filed 04/08/...</td>\n",
       "      <td>grant</td>\n",
       "      <td>support</td>\n",
       "      <td>test</td>\n",
       "      <td>711</td>\n",
       "      <td>gov.uscourts.dcd.147524.4.0.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>Case 1:11-cv-00696-RLW Document 22 Filed 04/28...</td>\n",
       "      <td>grant</td>\n",
       "      <td>opposition</td>\n",
       "      <td>test</td>\n",
       "      <td>711</td>\n",
       "      <td>gov.uscourts.dcd.147524.22.0.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1215 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt completion  \\\n",
       "0     Case 9:11-cv-80416-KLR Document 737 Entered on...       deny   \n",
       "1     Case 9:11-cv-80416-KLR Document 743 Entered on...       deny   \n",
       "2     IN THE UNITED STATES DISTRICT COURT\\nFOR THE S...       deny   \n",
       "3     I i .\\nt | | , : tl\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n...       deny   \n",
       "4     Case 1:08-cv-11908-RWZ Document 27\\n\\nFiled 09...      grant   \n",
       "...                                                 ...        ...   \n",
       "1210  Case 6:10-cv-00111-LED Document 333\\n\\nFiled 0...       deny   \n",
       "1211  Helio LLC v. Palm, Inc. Doc. 3\\n\\n \\n\\n \\n\\n \\...       deny   \n",
       "1212  Helio LLC v. Palm, lnc.\\n\\n60373/2021440.1\\n\\n...       deny   \n",
       "1213  Case 1:11-cv-00696-RLW Document 4 Filed 04/08/...      grant   \n",
       "1214  Case 1:11-cv-00696-RLW Document 22 Filed 04/28...      grant   \n",
       "\n",
       "      brief_type data_type  file_path                           file_name  \\\n",
       "0        support     train       1013  gov.uscourts.flsd.377721.737.0.txt   \n",
       "1     opposition     train       1013  gov.uscourts.flsd.377721.743.0.txt   \n",
       "2        support     train        909     gov.uscourts.mssd.95610.6.0.txt   \n",
       "3     opposition     train        909     gov.uscourts.mssd.95610.7.0.txt   \n",
       "4        support     train        507    gov.uscourts.mad.118796.27.0.txt   \n",
       "...          ...       ...        ...                                 ...   \n",
       "1210  opposition      test        621  gov.uscourts.txed.121829.333.0.txt   \n",
       "1211     support     train        574    gov.uscourts.cand.187342.3.0.txt   \n",
       "1212  opposition     train        574    gov.uscourts.cand.187342.9.0.txt   \n",
       "1213     support      test        711     gov.uscourts.dcd.147524.4.0.txt   \n",
       "1214  opposition      test        711    gov.uscourts.dcd.147524.22.0.txt   \n",
       "\n",
       "      labels  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          1  \n",
       "...      ...  \n",
       "1210       0  \n",
       "1211       0  \n",
       "1212       0  \n",
       "1213       1  \n",
       "1214       1  \n",
       "\n",
       "[1215 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decision2label(decision):\n",
    "    if  \"grant\" in decision:\n",
    "        return 1\n",
    "    elif \"deny\" in decision:\n",
    "        return 0\n",
    "    else:\n",
    "        exit(\"Invalid decision\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_function(briefs):\n",
    "     return tokenizer(briefs[\"prompt\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "TESTSET = \"/u3/oqcardos/motion_prediction/dataset/testset.csv\"\n",
    "\n",
    "testset = pd.read_csv(TESTSET, index_col=0)\n",
    "\n",
    "testset['labels'] = testset['completion'].apply(decision2label)\n",
    "\n",
    "train = testset.loc[testset['data_type'] == 'train']\n",
    "test = testset.loc[testset['data_type'] == 'test']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_train = Dataset.from_pandas(train, preserve_index=False)\n",
    "dataset_test = Dataset.from_pandas(test, preserve_index=False)\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = dataset_train\n",
    "dataset['test'] = dataset_test\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(200))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "\n",
    "print(dataset)\n",
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"tutorial_trainer\", evaluation_strategy=\"epoch\",  report_to=\"wandb\",)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native pytorch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"completion\",\"prompt\",\"brief_type\",\"data_type\", \"file_path\", \"file_name\"])\n",
    "#tokenized_datasets = tokenized_datasets.rename_column(\"completion\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=8)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# what does get_scheduler do?\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(0),\n",
       " 'input_ids': tensor([  101,  9060,   124,   131,  1405,   118,   172,  1964,   118,  3135,\n",
       "          1571,  1559,  1559,   118,   147, 22689, 11387, 15447,  2227,  5581,\n",
       "         25647,  1181,  5037,   120,  1479,   120,  1479,  8500,   122,  1104,\n",
       "          3164,  8500,  9949,   108, 22196,  1477,  7414, 12150, 10069, 23676,\n",
       "         13821,  9919,   141,  6258, 23313,  9741,  1942, 18732, 19556,  1942,\n",
       "           143,  9565,  7462, 23616,  9272,  9637,  2249,   141,  6258, 23313,\n",
       "          9741,  1942, 11345,  7118,  2069, 21065, 27451,  1592,  6110,  1784,\n",
       "           153,  3048, 17656, 11410,   150,  9565, 20595,  1708,  3066, 15969,\n",
       "          1658,   119,   117, 13823,  3121,  3101,   117,   191,   119,  6586,\n",
       "          9637,  9741, 14962, 18396,  9919, 23161, 20595,  9919, 15969, 12880,\n",
       "         15654, 21669, 11414, 12507,   117,   149,   119,   149,   119,   140,\n",
       "           119,   117,  3177, 13488, 26977,   119,   114,   114,   114,   114,\n",
       "           114,   114,   114,   114,   114,   114,   114,  3145,  6605,  1302,\n",
       "           119,   124,   131,  1405,   118,   172,  1964,   118,  4667,  1559,\n",
       "         18581,  2271, 11680, 11392, 15681,   787,   156,   152, 20923,  9025,\n",
       "         12150, 24805, 16972,   153, 10783, 11607, 21669, 17515,   787,   156,\n",
       "           150, 14697, 24805,   143,  9565,  8544, 10460,  9984,  2162,   156,\n",
       "         25810,  8271,  2069,  3663,   147,  2591,  2137, 23169, 11680,  1942,\n",
       "          9060,   124,   131,  1405,   118,   172,  1964,   118,  3135,  1571,\n",
       "          1559,  1559,   118,   147, 22689, 11387, 15447,  2227,  5581, 25647,\n",
       "          1181,  5037,   120,  1479,   120,  1479,  8500,   123,  1104,  3164,\n",
       "          8500,  9949,   108, 22196,  1495,   157, 19985, 17516, 11345, 18732,\n",
       "         15681, 11680, 11365,   157, 19985, 17516, 11345, 21646, 24162,  9565,\n",
       "         12150, 17444,  1708,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119, 25550,\n",
       "         15969, 23313, 15609, 21986, 21669, 11414,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   122, 12465,\n",
       "          1658,  2428, 27617,  2346, 27370,  2137,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   119,   119,   119,   119,   119,   119,   119,   119,   119,\n",
       "           119,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f353aaec8a4bed8accde84e2a30025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /u3/oqcardos/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Fri Feb  9 23:00:53 2024) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.48925619834710743}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
