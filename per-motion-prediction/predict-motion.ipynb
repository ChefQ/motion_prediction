{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7022/2043576970.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "from joblib import dump, load\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import (\n",
    "    Sequential as Seq,\n",
    "    Linear as Lin,\n",
    "    ReLU,\n",
    "    BatchNorm1d,\n",
    "    AvgPool1d,\n",
    "    Sigmoid,\n",
    "    Conv1d,\n",
    ")\n",
    "\n",
    "from deepsetmodel import *\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:1\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "testset = pd.read_csv('../dataset/paired_testset.csv', sep=',',index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator CountVectorizer from version 1.3.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/scratchB/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.3.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self,df,feature = 'tfdif', support_pipe = '../pipes/support-tfidf.joblib', opposition_pipe = '../pipes/oppose-tfidf.joblib', both =False, both_pipe = '../pipes/both-tfidf.joblib'):\n",
    "        self.df = df    \n",
    "        supports = self.df['support'].values\n",
    "        oppositions = self.df['opposition'].values\n",
    "        self.y = self.df['outcome'].values \n",
    "        # convert list of stings to list of lists of stings\n",
    "        supports = list(map(lambda x: ast.literal_eval(x), supports))\n",
    "        oppositions = list(map(lambda x: ast.literal_eval(x), oppositions))\n",
    "\n",
    "        self.max_len_brief = max(self.findMaxLen(supports),self.findMaxLen(oppositions))\n",
    "\n",
    "        if feature == 'tfdif':\n",
    "            if both == False:\n",
    "                support_pipe = load(support_pipe)\n",
    "                opposition_pipe = load(opposition_pipe)\n",
    "                getSupport = lambda x: self.stringsToTfidfs(x,support_pipe)\n",
    "                getOpposition = lambda x: self.stringsToTfidfs(x,opposition_pipe)\n",
    "            else:\n",
    "                both_pipe = load(both_pipe)\n",
    "                getSupport = lambda x: self.stringsToTfidfs(x,both_pipe)\n",
    "                getOpposition = lambda x: self.stringsToTfidfs(x,both_pipe)\n",
    "\n",
    "            self.supports = list(map( getSupport, supports))\n",
    "            self.oppositions = list(map( getOpposition, oppositions))\n",
    "\n",
    "        elif feature == 'embedding':\n",
    "            self.supports = []\n",
    "            self.oppositions = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.supports)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = 1.0 if self.y[idx] == 'grant' else 0.0\n",
    "\n",
    "        return self.supports[idx] , self.oppositions[idx] , y\n",
    "    \n",
    "    def findMaxLen(self,x):\n",
    "        max_len = 0\n",
    "        for i in range(len(x)):\n",
    "            row = x[i]\n",
    "            if len(row) > max_len:\n",
    "                max_len = len(row)\n",
    "        return max_len\n",
    "\n",
    "    def stringsToTfidfs(self,briefs,pipe):\n",
    "        tfidfs = torch.tensor(pipe.transform(briefs).toarray(),dtype=torch.float32)\n",
    "\n",
    "        num_padding = self.max_len_brief - tfidfs.shape[0]\n",
    "\n",
    "        padding = nn.ConstantPad2d((0, 0, 0, num_padding), 0)\n",
    "\n",
    "        tfidfs = padding(tfidfs)\n",
    "\n",
    "        tfidfs = tfidfs.T\n",
    "        return tfidfs\n",
    "    \n",
    "    def stringsToEmbeddings(self): \n",
    "        pass\n",
    "\n",
    "train_data = Data(testset[testset['data_type'] == 'train'], both = True)\n",
    "test_data = Data(testset[testset['data_type'] == 'test'], both = True) \n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input size for TFIDF is quite big, so instead of doubling the feauture size of the hidden layers.\n",
    "I simplely added it by 100 and 200 units respectively.\n",
    "\n",
    "How do i construct the input?\n",
    "- Should i have two different sets?\n",
    "- Or chug them all in the same set?\n",
    "\n",
    "\n",
    "I can not load multple models, because the TFIDF vectors creates large weights which in turn makes the model large\n",
    "\n",
    "There might be a way to mitigate this problem with:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TFIDF is quiet big so i may have to reduce the hiden layers width\n",
    "input_size = train_data.supports[0].shape[0]\n",
    "max_len_brief = train_data.max_len_brief\n",
    "\n",
    "# the latent space has the be atleast the size of the input\n",
    "\n",
    "models = {}\n",
    "optimizers = {}\n",
    "\n",
    "\n",
    "hidden1 = int(input_size /5)\n",
    "hidden2 = int(hidden1 / 4)\n",
    "hidden3 = int(hidden2 / 3)\n",
    "classify1 = int(hidden3 /2)\n",
    "\n",
    "#models[\"support\"] = DeepSets(input_size, max_len_brief , hidden1, hidden2, hidden3, classify1).to(device)\n",
    "\n",
    "models[\"opposition\"] = DeepSets(input_size,max_len_brief,  hidden1, hidden2, hidden3, classify1).to(device)\n",
    "\n",
    "\n",
    "latent_size = int(input_size / 10)\n",
    "hidden_size = latent_size\n",
    "output_size =  1\n",
    "\n",
    "\n",
    "#models[\"both\"] = MultiSetTransformer(input_size, latent_size, hidden_size, output_size ).to(device)\n",
    "\n",
    "## what does Bachnorm and conv1d work?\n",
    "\n",
    "#optimizers[\"suppport\"] = torch.optim.Adam(models[\"support\"].parameters(), lr=1e-4)\n",
    "optimizers[\"opposition\"] = torch.optim.Adam(models[\"opposition\"].parameters(), lr=1e-4)\n",
    "#optimizers[\"both\"] = torch.optim.Adam(models[\"both\"].parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader =iter(train_loader)\n",
    "supports, oppositions, y = next(loader)\n",
    "supports = supports.to(device)\n",
    "oppositions = oppositions.to(device)\n",
    "y = y.float()\n",
    "y = y.reshape(-1,1)\n",
    "y = y.to(device)\n",
    "\n",
    "outputs= models[\"both\"](supports, oppositions)\n",
    "loss_fn= nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = nn.Linear(input_size, latent_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ,Y = supports, oppositions\n",
    "X = X.reshape(X.shape[0], X.shape[2], X.shape[1])\n",
    "Y = Y.reshape(Y.shape[0], Y.shape[2], Y.shape[1])\n",
    "ZX , ZY = models[\"both\"].proj(X) , models[\"both\"].proj(Y)\n",
    "\n",
    "models[\"both\"].enc((ZX, ZY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"both\"].forward(X,Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training, validation, testing data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, total, batch_size, leave=False , datatype='support', loss_fn= nn.BCELoss()):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    sum_acc = 0.0\n",
    "\n",
    "    t = tqdm(enumerate(loader), total=total /batch_size, leave=leave)\n",
    "\n",
    "    for i, data in t:\n",
    "\n",
    "        supports, oppositions, y = data\n",
    "        supports = supports.to(device)\n",
    "        oppositions = oppositions.to(device)\n",
    "\n",
    "        y = y.float()\n",
    "        y = y.reshape(-1,1)\n",
    "        y = y.to(device)\n",
    "\n",
    "        if datatype == 'support':\n",
    "            outputs= model(supports)\n",
    "        elif datatype == 'opposition':\n",
    "            outputs= model(oppositions)\n",
    "        elif datatype == 'both':\n",
    "            outputs= model(supports, oppositions)\n",
    "\n",
    "        loss = loss_fn(outputs, y)\n",
    "\n",
    "        acc = ((outputs > 0.5) == y).sum().item()\n",
    "        sum_acc += acc\n",
    "        avg_acc =  acc /batch_size\n",
    "        \n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        t.set_description(f\"batch_loss_{datatype}: {loss.item():.4f} \\t| sum_loss_{datatype}: {sum_loss:.4f}\\n batch_accuracy_{datatype}: {avg_acc:.4f}\")\n",
    "        \n",
    "        t.refresh()\n",
    "    # what is the (i+1) for?\n",
    "        \n",
    "    return sum_loss  / len(loader.dataset) , sum_acc / len(loader.dataset)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader, total, batch_size, leave=False, datatype='support', loss_fn= nn.BCELoss()):\n",
    "    model.train()\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    t = tqdm(enumerate(loader), total=total /batch_size, leave=leave)\n",
    "    for i, data in t:\n",
    "\n",
    "\n",
    "        supports, oppositions, y = data\n",
    "        supports = supports.to(device)\n",
    "        oppositions = oppositions.to(device)\n",
    "        y = y.float()\n",
    "        y = y.reshape(-1,1)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if datatype == 'support':\n",
    "            outputs= model(supports)\n",
    "        elif datatype == 'opposition':\n",
    "            outputs= model(oppositions)\n",
    "        elif datatype == 'both':\n",
    "            outputs= model(supports, oppositions)\n",
    "\n",
    "        loss = loss_fn(outputs, y)\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        t.set_description(f\"batch_loss_{datatype}: {loss.item():.4f} \\t| sum_loss_{datatype}: {sum_loss:.4f}\")\n",
    "        t.refresh()\n",
    "\n",
    "    return sum_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219d1c57912846cb9239edd81f24f8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf3ba41b7474b2db34aa5a216d5ec88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dab2e82530a4f17aca2402c8fb53572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Training Loss:   0.0997\n",
      "           Validation Loss: 0.0883\n",
      "           Validation Accuracy: 0.5680\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7146c8e7bb1641f6b767ba30774b3871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20262beda59e474d9c80c483816d90d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Training Loss:   0.0376\n",
      "           Validation Loss: 0.1005\n",
      "           Validation Accuracy: 0.5680\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a20aa77e3d34b18b21e66a5b5748696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e68dbcc3e04aa9b6ac00e2778bd57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Training Loss:   0.0167\n",
      "           Validation Loss: 0.0966\n",
      "           Validation Accuracy: 0.5600\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8773fa727794ba596d3fd2f756f56ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652c8e6273c64cb9b5c534170b438457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Training Loss:   0.0104\n",
      "           Validation Loss: 0.1510\n",
      "           Validation Accuracy: 0.5080\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ed8b6551b74542a077692b1042ce64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc7fbfa7f3541c98bbd44b2625ec91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Training Loss:   0.0119\n",
      "           Validation Loss: 0.1484\n",
      "           Validation Accuracy: 0.5240\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fb1f8dcf084dc48d8a4e4a9a2637a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e2994c1e53493e8abe65888d9cb3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Training Loss:   0.0079\n",
      "           Validation Loss: 0.1440\n",
      "           Validation Accuracy: 0.5240\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63774ffc8e5412b8985e5a3349aa1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94437cd0a26b4163a2478ab3f47f85b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Training Loss:   0.0058\n",
      "           Validation Loss: 0.1398\n",
      "           Validation Accuracy: 0.5360\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd776be4bdb148c6bb3a2ed90fadc1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05eaeec492574996851ff0a4a8942dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Training Loss:   0.0128\n",
      "           Validation Loss: 0.1467\n",
      "           Validation Accuracy: 0.5520\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d3bd6116144967b1df048be0e8d71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132e1fbbe1334407ab6dcdf83096ef91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Training Loss:   0.0043\n",
      "           Validation Loss: 0.1657\n",
      "           Validation Accuracy: 0.5720\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6303b58ee64625b4cb58f84a144534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2166784c62dc43e993cebb603475fa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Training Loss:   0.0144\n",
      "           Validation Loss: 0.1944\n",
      "           Validation Accuracy: 0.4920\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e597368fa9de41c4a40798c36f272b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a37b900be5418dafa84dff1a2d5c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss:   0.0121\n",
      "           Validation Loss: 0.1465\n",
      "           Validation Accuracy: 0.5160\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea687e32398246e28dc24076d6a482a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8b613385064d47b24eadb6e059323a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Training Loss:   0.0122\n",
      "           Validation Loss: 0.1526\n",
      "           Validation Accuracy: 0.5560\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c79f84e2b342fda102581bb6d97045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0751dcf0ab7547b0998e5b199e41f61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Training Loss:   0.0113\n",
      "           Validation Loss: 0.1685\n",
      "           Validation Accuracy: 0.5000\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3578ecd3e1642bfba90a7f500cf897f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ec20cde72d4473bae3b248bc8bee01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Training Loss:   0.0069\n",
      "           Validation Loss: 0.1496\n",
      "           Validation Accuracy: 0.4840\n",
      "Stale epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e24ae17c92a415aa7eba869d1f8b254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31.25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "n_epochs = 30\n",
    "stale_epochs = 0\n",
    "best_valid_loss = 99999\n",
    "patience = 15\n",
    "t = tqdm(range(0, n_epochs))\n",
    "\n",
    "key = \"opposition\"\n",
    "for epoch in t:\n",
    "    avg_loss = train(\n",
    "        model=models[key], \n",
    "        optimizer=optimizers[key], \n",
    "        loader=train_loader, \n",
    "        total=len(train_data), \n",
    "        batch_size=batch_size, \n",
    "        leave=bool(epoch == n_epochs - 1) \n",
    "    )\n",
    "    \n",
    "    \n",
    "    valid_loss, valid_acc = test(\n",
    "        model=models[key],\n",
    "        loader=test_loader, \n",
    "        total=len(test_data), \n",
    "        batch_size=batch_size, \n",
    "        leave=bool(epoch == n_epochs - 1),\n",
    "        datatype=key\n",
    "    )\n",
    "\n",
    "    print(\"Epoch: {:02d}, Training Loss:   {:.4f}\".format(epoch, avg_loss))\n",
    "    print(\"           Validation Loss: {:.4f}\".format(valid_loss))\n",
    "    print(\"           Validation Accuracy: {:.4f}\".format(valid_acc))\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # modpath = osp.join(\"deepsets_best.pth\")\n",
    "        # print(\"New best model saved to:\", modpath)\n",
    "        # torch.save(model.state_dict(), modpath)\n",
    "        stale_epochs = 0\n",
    "    else:\n",
    "        print(\"Stale epoch\")\n",
    "        stale_epochs += 1\n",
    "    if stale_epochs >= patience:\n",
    "        print(\"Early stopping after %i stale epochs\" % patience)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: comm>=0.1.3 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from ipywidgets) (5.14.1)\n",
      "Collecting widgetsnbextension~=4.0.9 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jupyterlab-widgets~=3.0.9 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.9/214.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/oqcardoso/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.1 jupyterlab-widgets-3.0.9 widgetsnbextension-4.0.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32]), torch.Size([1, 32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.squeeze().shape, y.reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1], device='mps:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# output = loss(input, target)\n",
    "# output.backward()\n",
    "# # Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0984, -0.8004, -1.2692,  0.0133,  0.8583],\n",
       "        [-2.5194, -1.6218,  2.3816, -0.0952, -0.3689],\n",
       "        [ 0.6391, -1.4880,  0.7888, -0.8413,  1.8981]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6053, 0.0873, 0.1340, 0.0427, 0.1307],\n",
       "        [0.1373, 0.1314, 0.5137, 0.0558, 0.1618],\n",
       "        [0.3586, 0.4966, 0.0283, 0.0868, 0.0297]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, 2, requires_grad=True)\n",
    "target = torch.rand(3, 2, requires_grad=False)\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-scatter in /Users/oluwaseuncardoso/Desktop/Projects/venv/lib/python3.9/site-packages (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 4., 3., 2.],\n",
      "        [0., 2., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "from torch_scatter import scatter_mean\n",
    "src = torch.Tensor([[2, 0, 4, 4, 3], [0, 2, 1, 3, 4]])\n",
    "index = torch.tensor([[4, 4, 4, 2, 3], [1,1,1,1, 1]])\n",
    "out = src.new_zeros((2, 5))\n",
    "\n",
    "scatter_mean(src, index, out=out)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.0000],\n",
      "        [3.4000],\n",
      "        [2.0000],\n",
      "        [2.6000]])\n"
     ]
    }
   ],
   "source": [
    "src = torch.Tensor([[10,10, 10, 5, 10], [4, 4, 4, 2, 3] , [0, 2, 1, 3, 4] , [2, 0, 4, 4, 3] ])\n",
    "\n",
    "index = torch.tensor([0, 0, 0, 0, 0])\n",
    "\n",
    "out = scatter_mean(src, index)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.0000, 3.4000, 2.0000, 2.6000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(src, dim=-1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2+4+4+3)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "src = torch.tensor([[2, 0, 4, 4, 3], [0, 2, 1, 3, 4]])\n",
    "index = torch.tensor([0, 0, 0, 0, 0])\n",
    "\n",
    "out = scatter_mean(src, index)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
