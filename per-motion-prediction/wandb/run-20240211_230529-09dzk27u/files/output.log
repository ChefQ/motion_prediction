Accuracy: {'accuracy': 0.532871972318339}
loss : 0.6954813261289854
Accuracy: {'accuracy': 0.5086505190311419}
loss : 0.6954603775127514
Accuracy: {'accuracy': 0.4671280276816609}
loss : 0.709547799986762
Accuracy: {'accuracy': 0.48788927335640137}
loss : 0.7105869215887946
Accuracy: {'accuracy': 0.4671280276816609}
loss : 0.7092904206868764
Accuracy: {'accuracy': 0.4602076124567474}
loss : 0.7084596479261244
Accuracy: {'accuracy': 0.48788927335640137}
loss : 0.7088258169792794
Accuracy: {'accuracy': 0.44982698961937717}
loss : 0.712407065404428
Accuracy: {'accuracy': 0.47058823529411764}
loss : 0.7121859692238472
Accuracy: {'accuracy': 0.5224913494809689}
loss : 0.7024535891172048
Accuracy: {'accuracy': 0.5224913494809689}
loss : 0.6874891277906057
Accuracy: {'accuracy': 0.5051903114186851}
loss : 0.6891899479402078
Accuracy: {'accuracy': 0.4844290657439446}
loss : 0.7171337153460529
Accuracy: {'accuracy': 0.47750865051903113}
loss : 0.7022179155736357
Accuracy: {'accuracy': 0.4844290657439446}
loss : 0.7068711084288519
Accuracy: {'accuracy': 0.5224913494809689}
loss : 0.6951699369662517
Accuracy: {'accuracy': 0.47750865051903113}
loss : 0.7119753489623198
Accuracy: {'accuracy': 0.4809688581314879}
loss : 0.7028458182876175
Accuracy: {'accuracy': 0.4532871972318339}
loss : 0.7078140345779625
Accuracy: {'accuracy': 0.4463667820069204}
loss : 0.7155927500209293
Accuracy: {'accuracy': 0.4671280276816609}
loss : 0.6959295949420413
Accuracy: {'accuracy': 0.5190311418685121}
loss : 0.6917436074566197
Accuracy: {'accuracy': 0.5086505190311419}
loss : 0.6958423724045625
Accuracy: {'accuracy': 0.45674740484429066}
loss : 0.703845109488513
Accuracy: {'accuracy': 0.47058823529411764}
loss : 0.7090447013442581
Accuracy: {'accuracy': 0.47750865051903113}
loss : 0.7039684991578798
Accuracy: {'accuracy': 0.5121107266435986}
loss : 0.7022881942826349
Accuracy: {'accuracy': 0.5259515570934256}
loss : 0.6991032941921337
Accuracy: {'accuracy': 0.4463667820069204}
loss : 0.7046458109005077
Accuracy: {'accuracy': 0.43944636678200694}
loss : 0.7076374823982651
Accuracy: {'accuracy': 0.5294117647058824}
loss : 0.6958726402875539
Accuracy: {'accuracy': 0.44982698961937717}
loss : 0.7068400463542422
Accuracy: {'accuracy': 0.43944636678200694}
loss : 0.7018132274215286
Accuracy: {'accuracy': 0.48788927335640137}
loss : 0.6974996328353882
Accuracy: {'accuracy': 0.44982698961937717}
loss : 0.7035965146245183
Accuracy: {'accuracy': 0.49480968858131485}
loss : 0.7041954333717758
Accuracy: {'accuracy': 0.5224913494809689}
loss : 0.7003777445973577
Accuracy: {'accuracy': 0.4671280276816609}
loss : 0.7072551153801583
Accuracy: {'accuracy': 0.5086505190311419}
loss : 0.7007748259080423
Accuracy: {'accuracy': 0.5086505190311419}
loss : 0.7003410893517572
Accuracy: {'accuracy': 0.5155709342560554}
loss : 0.7050032084052628
Accuracy: {'accuracy': 0.4844290657439446}
loss : 0.6949410615740595
Accuracy: {'accuracy': 0.5086505190311419}
loss : 0.7015052486110378
Accuracy: {'accuracy': 0.46366782006920415}
loss : 0.7035250905397776
Accuracy: {'accuracy': 0.5051903114186851}
loss : 0.6980509532464517
Accuracy: {'accuracy': 0.5086505190311419}
loss : 0.7094109493332941
Accuracy: {'accuracy': 0.47750865051903113}
loss : 0.7103091687769503
Accuracy: {'accuracy': 0.5051903114186851}
loss : 0.6951387137980074
Accuracy: {'accuracy': 0.4671280276816609}
loss : 0.701472016605171
Accuracy: {'accuracy': 0.46366782006920415}
loss : 0.7121252498111209
Accuracy: {'accuracy': 0.5363321799307958}
loss : 0.701750901905266
Accuracy: {'accuracy': 0.5397923875432526}
loss : 0.7006808519363403
Accuracy: {'accuracy': 0.532871972318339}
loss : 0.6914748984414179
Accuracy: {'accuracy': 0.43944636678200694}
loss : 0.7088710762359001
Accuracy: {'accuracy': 0.4913494809688581}
loss : 0.6968119966017233
Accuracy: {'accuracy': 0.47058823529411764}
loss : 0.708636873477214
Accuracy: {'accuracy': 0.5432525951557093}
loss : 0.6913892868402842
Accuracy: {'accuracy': 0.45674740484429066}
loss : 0.7006267615266748
Accuracy: {'accuracy': 0.47058823529411764}
loss : 0.7032859228752755
Accuracy: {'accuracy': 0.4740484429065744}
loss : 0.7005294255308203
Accuracy: {'accuracy': 0.4982698961937716}
loss : 0.6988724888982
Accuracy: {'accuracy': 0.48788927335640137}
loss : 0.7045363171680553
Accuracy: {'accuracy': 0.43944636678200694}
loss : 0.7062492225621197
Accuracy: {'accuracy': 0.5190311418685121}
loss : 0.6956433669940846
Accuracy: {'accuracy': 0.4359861591695502}
loss : 0.7138474631953884
Accuracy: {'accuracy': 0.4602076124567474}
loss : 0.707028455025441
Accuracy: {'accuracy': 0.48788927335640137}
loss : 0.7004982829093933
Accuracy: {'accuracy': 0.4844290657439446}
loss : 0.7098082030141676
Accuracy: {'accuracy': 0.4844290657439446}
loss : 0.7067735903971905
Accuracy: {'accuracy': 0.4913494809688581}
loss : 0.6998018606289013
Accuracy: {'accuracy': 0.5294117647058824}
loss : 0.6965211919836096
Accuracy: {'accuracy': 0.45674740484429066}
loss : 0.7092276586068643
Accuracy: {'accuracy': 0.4844290657439446}
loss : 0.7076545261047982
Accuracy: {'accuracy': 0.4532871972318339}
loss : 0.7072591298335308
Accuracy: {'accuracy': 0.5190311418685121}
loss : 0.6997229070276827
Accuracy: {'accuracy': 0.48788927335640137}
loss : 0.7167926575686481
Accuracy: {'accuracy': 0.49480968858131485}
loss : 0.6960828368728226
Accuracy: {'accuracy': 0.5051903114186851}
loss : 0.6995727403743847
Accuracy: {'accuracy': 0.4602076124567474}
loss : 0.7132385292568723
Accuracy: {'accuracy': 0.4359861591695502}
loss : 0.7126820087432861
Accuracy: {'accuracy': 0.49480968858131485}
loss : 0.7003473413957132
Accuracy: {'accuracy': 0.4359861591695502}
loss : 0.7101579727353277
Accuracy: {'accuracy': 0.4671280276816609}
loss : 0.7087003862535631
Accuracy: {'accuracy': 0.4463667820069204}
loss : 0.7130266959602768
Accuracy: {'accuracy': 0.46366782006920415}
loss : 0.6901473781547031
Accuracy: {'accuracy': 0.5051903114186851}
loss : 0.6985394841915852
Accuracy: {'accuracy': 0.5017301038062284}
loss : 0.7028756624943501
Accuracy: {'accuracy': 0.47058823529411764}
loss : 0.7091114569354702
Accuracy: {'accuracy': 0.48788927335640137}
loss : 0.7049141429566048
Accuracy: {'accuracy': 0.5121107266435986}
loss : 0.6954671998281736
Accuracy: {'accuracy': 0.5294117647058824}
loss : 0.6944294298017347
Accuracy: {'accuracy': 0.4740484429065744}
loss : 0.7061636915078034
Accuracy: {'accuracy': 0.532871972318339}
loss : 0.692397811928311
Accuracy: {'accuracy': 0.49480968858131485}
loss : 0.691688405500876
Accuracy: {'accuracy': 0.5432525951557093}
loss : 0.6855770865002194
Accuracy: {'accuracy': 0.4671280276816609}
loss : 0.7120942715052012
Accuracy: {'accuracy': 0.4671280276816609}
loss : 0.7080866533356744
Accuracy: {'accuracy': 0.5121107266435986}
loss : 0.7024958681415867
Accuracy: {'accuracy': 0.4532871972318339}
loss : 0.7121156228555215
Accuracy: {'accuracy': 0.49480968858131485}
loss : 0.6948196356360977
Accuracy: {'accuracy': 0.4844290657439446}
loss : 0.7002224293915001
Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 860,160 || all params: 7,111,528,448 || trainable%: 0.012095290151611583